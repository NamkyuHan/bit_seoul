#분류
#실습 LSTM으로 이미지 처리하기
#OneHotEncoding
#sklearn one hot encoding : 하나의 값만 True, 나머지는 모두 False

# 1. 데이터
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist

#(x_train, y_train), (x_test, y_test)에 케라스 안에 있는 데이터셋 mnist 저장
(x_train, y_train), (x_test, y_test) = mnist.load_data()

#빈칸 그대로 두지 말고 원래 shape처럼 공백 : 넣어주기!
# x_predict = x_test[:10, :, :] 내가 답을 유추해 볼건데 10개 까지 볼거야(슬라이싱)
# y_real = y_test[:10, :, :] 내가 답을 유추해 볼거면 답이 있어야 겠지? 정해져 있는 그 답도 10개 까지 볼거야(슬라이싱)
x_predict = x_test[:10, :, :]

# 프린트 해서 값을 쉐이프를 확인해보자
# x_train, x_test, y_train, y_test
# 쉐이프 확인 했으면 헷갈리지 않게 옆에다가 잘 써주자
# print(x_train[0]), print(y_train[0]) = 이건 왜? 원래 매칭되어 있는 값을 내눈으로 한번 더 볼려고
print(x_train.shape, x_test.shape) #(60000, 28, 28) (10000, 28, 28)
print(y_train.shape, y_test.shape) #(60000, ) (10000, )
print(x_train[0])
print(y_train[0])

#다중 분류 데이터 전처리 (1).OneHotEncoding
# 1. sklearn을 통해 임포트 할 수도 있다
# from sklearn.preprocessing import OneHotEncoder
# enc(변수설정) = OneHotEncoder()(OneHotEncoder 대입)
# enc.fit(Y_class) (fit 설정 x,y 데이터 train 있으면 train으로 대입)
# 2. OneHotEncodeing  인코딩은 아래 코드와 같이 케라스에서 제공하는 “to_categorical()”로 쉽게 처리할 수 있습니다
from tensorflow.keras.utils import to_categorical
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)


# OneHotEncodeing으로 변경된 y_train, y_test 쉐이프 확인하기
# 대입되어 있는 y_train 값 확인하기
print("y_train shape : ", y_train.shape) #(60000, 10)
print("y_test shape : ", y_test.shape) #(10000, 10)
print("y_train data : ", y_train[0]) #[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]


# LSTM에 집어넣기 위해 3차원으로 x_train, x_test, x_predict reshape하기
# x_predict은 x_test를 통해 위에 10개까지 본다고 설정해 놨기 때문에 = x_predict = x_test[:10, :, :] / (10, 196, 4)로 3차원 만들기 
x_train = x_train.reshape(60000, 196, 4).astype('float32')/255. #= 781, 1 같음 28*28 = 784 1개로 쪼갤거야 4개로 쪼개면 784를 4로 나누면 됨 (194, 4)
x_test = x_test.reshape(10000, 196, 4).astype('float32')/255.


# 3차원으로 변경된 x_train도 확인해 볼거야 y_train[0]도 0을 봤으니 x_train도 [0]을 봐야 하겠지?
print("x_train data : ", x_train[0])
'''
 [[0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.01176471 0.07058824 0.07058824 0.07058824]
 [0.49411765 0.53333336 0.6862745  0.10196079]
 [0.6509804  1.         0.96862745 0.49803922]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.11764706 0.14117648 0.36862746 0.6039216 ]
 [0.6666667  0.99215686 0.99215686 0.99215686]
 [0.99215686 0.99215686 0.88235295 0.6745098 ]
 [0.99215686 0.9490196  0.7647059  0.2509804 ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.19215687]
 [0.93333334 0.99215686 0.99215686 0.99215686]
 [0.99215686 0.99215686 0.99215686 0.99215686]
 [0.99215686 0.9843137  0.3647059  0.32156864]
 [0.32156864 0.21960784 0.15294118 0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.07058824]
 [0.85882354 0.99215686 0.99215686 0.99215686]
 [0.99215686 0.99215686 0.7764706  0.7137255 ]
 [0.96862745 0.94509804 0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.3137255  0.6117647  0.41960785 0.99215686]
 [0.99215686 0.8039216  0.04313726 0.        ]
 [0.16862746 0.6039216  0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.05490196 0.00392157 0.6039216 ]
 [0.99215686 0.3529412  0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.54509807]
 [0.99215686 0.74509805 0.00784314 0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.04313726]
 [0.74509805 0.99215686 0.27450982 0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.13725491 0.94509804 0.88235295 0.627451  ]
 [0.42352942 0.00392157 0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.31764707 0.9411765  0.99215686]
 [0.99215686 0.46666667 0.09803922 0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.1764706  0.7294118 ]
 [0.99215686 0.99215686 0.5882353  0.10588235]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.0627451 ]
 [0.3647059  0.9882353  0.99215686 0.73333335]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.9764706  0.99215686 0.9764706 ]
 [0.2509804  0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.18039216 0.50980395]
 [0.7176471  0.99215686 0.99215686 0.8117647 ]
 [0.00784314 0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.15294118 0.5803922  0.8980392  0.99215686]
 [0.99215686 0.99215686 0.98039216 0.7137255 ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.09411765 0.44705883]
 [0.8666667  0.99215686 0.99215686 0.99215686]
 [0.99215686 0.7882353  0.30588236 0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.09019608 0.25882354 0.8352941  0.99215686]
 [0.99215686 0.99215686 0.99215686 0.7764706 ]
 [0.31764707 0.00784314 0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.07058824 0.67058825]
 [0.85882354 0.99215686 0.99215686 0.99215686]
 [0.99215686 0.7647059  0.3137255  0.03529412]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.21568628 0.6745098  0.8862745  0.99215686]
 [0.99215686 0.99215686 0.99215686 0.95686275]
 [0.52156866 0.04313726 0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.53333336 0.99215686 0.99215686 0.99215686]
 [0.83137256 0.5294118  0.5176471  0.0627451 ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]]
'''
#2. 모델 구성
from tensorflow.keras.models import Sequential #Sequential 임포트 하고
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, LSTM # 이거 네개도 사용할 거니까 임포트
model=Sequential()
model.add(LSTM(20, activation='relu', input_shape=(196, 4)))
model.add(Dense(300, activation='relu'))
model.add(Dense(200, activation='relu'))
model.add(Dense(150, activation='relu'))
model.add(Dense(90))
model.add(Dense(30))
model.add(Dense(10, activation='softmax')) 
#(2). 다중 분류의 output layer의 활성화함수는 softmax를 쓴다.

model.summary()

#3. 컴파일 훈련
# 다중분류에선 반드시 loss를 categorical_crossentropy로 쓴다. 이걸로 이제 accuracy를 잡아줄 수 있다.
from tensorflow.keras.callbacks import EarlyStopping # 조기 종료
early_Stopping = EarlyStopping(monitor='loss', patience=10, mode='auto')
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) 
# loss='categorical_crossentropy' 이거 꼭 들어감 #모든 값을 합친 것은 1이 된다 acc 때문에
# 'mse' 쓰려면? mean_squared_error 이것도 가능
model.fit(x_train, y_train, epochs=1, batch_size=32, verbose=1, validation_split=0.2, callbacks=[early_Stopping]) #원래 배치 사이즈의 디폴트는 32

#4. 평가 예측
# model.evaluate를 통해 결과값을 예측해보자 여기에는 테스트 값만 들어가야 함
loss, acc = model.evaluate(x_test, y_test, batch_size=32)
print("loss : ", loss)
print("acc : ", acc)

# 이미 우리는 train, test로 나눠서 x,y 둘다 훈련을 시켰음
# x_train, x_test를 reshape 했기 때문에 x_predict도 동일하게 형태를 바꿔준다
x_predict = x_predict.reshape(10, 196, 4).astype('float32')/255.
print("x_predict : ", x_predict)

# 훈련을 통해 나온 x_predict 값을 y_predict에 넣자
y_predict = model.predict(x_predict)
# y_train, y_test를 OneHotEncoding을 했으니 데이터 복호화 진행
y_predict = np.argmax(y_predict, axis=1)
# 가장 최근의 y_test 사이즈를 확인한 후 복호화 진행
y_real = np.argmax(y_test[:10, :], axis=1)
print('실제값(y_real) : ', y_real)
print('예측값(y_predict) : ', y_predict)


'''
실습 1. test 데이터를 10개 가져와서 predict 만들것
-원핫 인코딩을 원복할 것
print('실제값 : ', y_real) 결과 : [3 4 5 2 9 1 3 9 0]
print('예측값 : ', y_predict_re) 결과 : [3 4 5 2 9 1 3 9 1]
y 값이 원핫 인코딩 되어있음
이걸 원복 시켜야 한다

실습 2. 모델 es적용 얼리스탑, 텐서보드도 넣을것
'''
